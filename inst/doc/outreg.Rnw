\batchmode
\makeatletter
\def\input@path{{/home/pauljohn/SVN/rgroup/trunk/rockchalk/rockchalk.svnex/vignettes//}}
\makeatother
\documentclass[english,noae]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{url}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{Sweavel}
<<echo=F>>=
  if(exists(".orig.enc")) options(encoding = .orig.enc)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\VignetteIndexEntry{Using rockchalk}

\usepackage{Sweavel}
\usepackage{graphicx}
\usepackage{color}

\usepackage[samesize]{cancel}



\usepackage{ifthen}

\makeatletter

\renewenvironment{figure}[1][]{%

 \ifthenelse{\equal{#1}{}}{%

   \@float{figure}

 }{%

   \@float{figure}[#1]%

 }%

 \centering

}{%

 \end@float

}

\renewenvironment{table}[1][]{%

 \ifthenelse{\equal{#1}{}}{%

   \@float{table}

 }{%

   \@float{table}[#1]%

 }%

 \centering

%  \setlength{\@tempdima}{\abovecaptionskip}%

%  \setlength{\abovecaptionskip}{\belowcaptionskip}%

% \setlength{\belowcaptionskip}{\@tempdima}%

}{%

 \end@float

}


%\usepackage{listings}
% Make ordinary listings look as if they come from Sweave
\lstset{tabsize=2, breaklines=true,style=Rstyle}

% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\def\Sweavesize{\scriptsize} 
\def\Rcolor{\color{black}} 
\def\Rbackground{\color[gray]{0.90}}

\makeatother

\usepackage{babel}
\begin{document}

\title{Using rockchalk for Quick \& Consistent Regression Presentations}


\author{Paul Johnson}

\maketitle
% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\SweaveOpts{ae=F,nogin=T}

<<Roptions, echo=F>>=
options(device = pdf)
options(width=160, prompt=" ", continue="  ")
options(useFancyQuotes = FALSE) 
@


\section{Introduction}

The rockchalk package is an agglomeration of functions that I need
when I'm teaching about regression. The functions here divide into
three categories. 
\begin{enumerate}
\item Functions that help me prepare lectures. The function to create \LaTeX{}
tables from regression output, outreg, falls into this category. It
speeds up the preparation of lectures immensely to include table generating
code that ``just works'' with R output. Some functions in R are
very hard to use and get right consistently, especially where 3 dimensional
plotting is concerned. That's where functions like mcGraph1, mcGraph2,
mcGraph3, and plotPlane come in handy. These don't do any work that
is particularly original, but they do help to easily make the multidimensional
plots that turn out ``about right'' most of the time. 
\item Functions simplify vital chores that are difficult for regression
students. I often ask students to plot several regression lines, ``one
for each sub-group of respondents,'' and this sometimes proves frustrating.
The function plotSlopes is offered as my suggestion for creating interaction
plots of ``simple slopes''. This handles the work of calculating
predicted values and drawing them for several possible values of a
third variable. plotPlane is along the same line. If students find
that useful, they can then use the examples to build up more complicated
drawings.
\item Functions that people often ask for, even if they might be unwise.
A funtion to estimate a ``standardised regression'' is offered.
Although that is clearly unwise (in the eyes of many), some folks
still want to calculate ``beta weights.'' Some functions, such as
meanCenter and residualCenter, are offered not because I need those
tools, but because other people propose them the use of my students.
Those procedures are, possibly, not truly helpful and in order to
demonstrate that fact, I have to provide the functions.
\end{enumerate}

\section{Some outreg Examples.}

outreg was a function in search of a package for a long time. I didn't
bother to build rockchalk until I had some other worthwhile functions.
So it seems appropriate to start with outreg.

On May 8, 2006, Dave Armstrong, a political science PhD student at
University of Maryland, posted a code snippet in r-help that demonstrated
one way to use the ``cat'' function from R to write \LaTeX{} markup.
That gave me the idea to write a \LaTeX{} output scheme that would
help create some nice looking term and research papers. I'd been frustrated
with the \LaTeX{} output from other R functions. I needed a table-maker
to include all of the required information in a regression table without
including a lot of chaff (in my opinion). I don't want both the standard
error of b and the t value, I never want p values, I need stars for
the significant variables, and I want a minimally sufficient set of
summary statistics. In 2006, there was no function that met those
needs.

<<echo=F>>=
library(rockchalk)
@

These models are created with some simulated data.

<<echo=T>>=
set.seed(1234)
x1 <- rnorm(100)
x2 <- rnorm(100)
y1 <- 5*rnorm(100) - 3*x1 + 4*x2
y2 <- rnorm(100)+5*x2
dat <- data.frame(x1, x2, y1, y2)
rm (x1, x2, y1, y2)
m1 <- lm (y1~x1, data=dat)
m2 <- lm (y1~x2, data=dat)
m3 <- lm (y1 ~ x1 + x2, data=dat)
myilogit <- function(x) exp(x)/(1 + exp(x))
y3 <- rbinom(100, size=1, p=myilogit(scale(dat$y1)))
gm1 <- glm(y3~x1 + x2, data=dat)
@

In each of the floating tables, I have presented an example use of
the ``outreg'' function along with the regression table that it
creates.

\begin{table}
\caption{My One Tightly Printed Regression\label{tab:Tab1}}


<<outreg10, results=tex, echo=F>>=
outreg(m1)
@
\end{table}


Table \ref{tab:Tab1} displays the default output, without any special
options. The command is

<<results=hide>>=
<<outreg10>>
@, 

\begin{table}
\caption{My Spread Out Regressions\label{tab:Tab2}}


<<outreg20, results=tex, echo=F>>=
outreg(m1, tight=FALSE, modelLabels=c("Fingers"))
@
\end{table}


In the literature, regression tables are sometimes presented in a
tight column format, with the estimates of the coefficients and standard
errors ``stacked up'' to allow multiple models side by side, while
sometimes they are printed with separate columns for the coefficients
and standard errors. The outreg option tight=F provides the two column
style. In Table \ref{tab:Tab2}, I've also used the argument modelLabels
to insert the word ``Fingers'' above the regression model. The command
that produces the table is

<<results=hide>>=
<<outreg20>>
@

\begin{table}
\caption{My Two Linear Regressions Tightly Printed\label{tab:Tab3}}


(a) Tightly Formatted Columns

<<outreg30, results=tex, echo=FALSE>>=
outreg(list(m1,m2), modelLabels=c("Mine","Yours"), varLabels = list(x1="Billie"))
@

(b) Two Columns Per Regression Model

<<outreg33, results=tex, echo=FALSE>>=
outreg(list(m1,m2), tight=FALSE,  modelLabels=c("Mine","Yours"), varLabels = list(x1="Billie"))
@
\end{table}


The outreg function can present different models in a single table,
as we see in Table \ref{tab:Tab3}. The default output uses the tight
format, so there is no need to specify that explicitly. In part (a)
of Table \ref{tab:Tab3}, we have tightly formatted columns of regression
output that result from this command:

<<results=hide>>=
<<outreg30>>
@\\
To my eye, there is something pleasant about the less-tightly-packed
format, as illustrated in part (b) of Table \ref{tab:Tab3}. Note
that the only difference in the commands that produce those tables
is the insertion of tight=FALSE.

<<results=hide>>=
<<outreg33>>
@

\begin{table}
\caption{My Three Linear Regressions in a Tight Format\label{tab:3tight}}


<<outreg35, results=tex, echo=F>>=
outreg(list(m1,m2,m3), modelLabels=c("A","B","C"), varLabels = list(x1="I Forgot x1", x2="He Remembered x2"))
@
\end{table}
In addition to using modelLables to provide headings for the 2 models,
the other argument that was used in Table is \ref{tab:Tab3} varLabels.
It is often a problem that the variables in the R program are terse,
while a presentation must have a full name. So in Table \ref{tab:Tab3},
I've demonstrated how to replace the variable name x1 with the word
``Billie''. Any of the predictor variables can be re-named in this
way. Another usage of varLabels is offered in an example with three
models in Table \ref{tab:3tight}, which is a result of

<<results=hide>>=
<<outreg35>>
@\\
As one can see, outreg gracefully handles the situation in which variables
are inserted or removed from a fitted model.

\begin{table}
\caption{Three Regressions in the Spread out Format\label{tab:3RegNotTIght}}


<<results=tex, echo=F>>=
outreg(list(m1,m2,m3), tight=F, modelLabels=c("I Love love love really long titles","Hate Long","Medium"))
@
\end{table}


I have not bothered with some fine points of \LaTeX{} table formatting.
I also have not worried about the problem of restricting columns to
use the exact same amount of horizontal space. In Table \ref{tab:3RegNotTIght},
we have regression output which is, in my opinion, completely acceptable
for inclusion in a presentation or conference paper. Because the model
labels are not equal in length, the columns are not equally sized.
That is not a concern for me, at the moment, but I imagine it might
be a concern for somebody. Perhaps, at some point, I may come back
and deal with the problem that decimal values within columns should
be vertically aligned (at least as an option). I don't want to make
outreg output dependent on additional \LaTeX{} packages.

\begin{table}
\caption{Combined OLS and GLM Estimates\label{tab:Combined-OLSGLM}}


<<outreg70, results=tex, echo=F>>=
outreg(list(m1,gm1),modelLabels=c("OLS:y1","GLM: Categorized y1"))
@
\end{table}


Another feature of outreg is that it can present the estimates of
different kinds of models. It can present the estimates from R's lm
and glm functions in a single table. Consider Table \ref{tab:Combined-OLSGLM},
which resulted from the command

<<results=hide>>=
<<outreg70>>
@

At one time, I was working on a similar presentation for mixed models
estimated by lme4, but I stopped that effort because the lme4 package
was changing rapidly and the format of its returned objects was not
stable enough for a finalized presentation format. Eventually, I will
include a method to display those mixed models. 


\section{plotPlane and plotCurves}

The goal of plotPlane and plotCurves is to speed up the process of
visualization in regression analysis. plotPlane offers a 3 dimensional
drawing that uses R's persp function to do the heavy lifting. plotCurves
tries to press that same information into a two dimensional display
by drawing curves for several different values of a moderator variable.
Both plotPlane and plotCurves allow nonlinear terms in the regression
model that is being plotted. In that sense, they are more similar
to R's own termplot function than to other similar tools. 

In this section, I create a new dependent variable y5 and then put
the fitted model through the plotSlopes and plotPlane functions.

<<>>=
dat$y5 <- with(dat, -3*x1 + 0.5*log(x2^2) + 1.1*x2 + 2.2 *x1 * x2 + 3*rnorm(100)) 
m5 <- lm (y5 ~ log(x2*x2) + x1 * x2, data=dat)
@

As illustrated in Figure \ref{fig:pcps10}, plotPlane allows the depiction
of a 3 dimensional plane that ``sits'' in the cloud of data points.
The variables that are not explicitly pictured in the plotPlane figure
are set to reference values. As illustrated in Figure \ref{fig:pcps20},
plotCurves is a 2 dimensional depiction of the same information. 

\begin{figure}
<<pcps10, fig=T, height=4, width=4>>=
plotPlane(m5, plotx1="x1", plotx2="x2")
@

<<results=tex>>=
outreg(m5, tight=FALSE)
@

\caption{plotPlane\label{fig:pcps10}}
\end{figure}


\begin{figure}
<<pcps20, fig=T, height=6, width=6>>=
plotCurves(m5, plotx="x1", modx="x2")
@

\caption{plotCurves\label{fig:pcps20}}
\end{figure}



\section{Plot and Test Simple Slopes}

In psychology, methodologists have recommended the analysis of ``simple
slopes'' to depict the effect of several variables in a 2 dimensional
plot. This is most often of interest in the analysis of regression
models with interactive terms. Suppse the fitted model is, 
\begin{equation}
\hat{y}_{i}=\hat{b}_{0}+\hat{b}_{1}x1_{i}+\hat{b}_{2}x2_{i}+\hat{b}_{3}x1_{i}x2_{i}+x3_{i}.As
\end{equation}
The idea is to consider the effect of $x1$ on $y$ for several values
of $x2$, keeping $x3_{i}$ set at some reference value (the mean
for numeric variables). As a follow-up, one wants to test whether
the plotted effects are statistically significantly different from
zero. 

This is only truly interesting when there are interaction effects,
of course, but I begin with a simple linear model. Recall that

\begin{lstlisting}
m3 <- lm (y1 ~ x1 + x2, data=dat)
\end{lstlisting}


Figure \ref{fig:ps10} illustrates the plotSlopes function for two
use cases. The first is

\begin{lstlisting}
plotSlopes(m3, plotx="x1", modx="x2", xlab="x1 is a Continuous Predictor")
\end{lstlisting}


\noindent The plotx argument is variable x1, meaning that x1 will
be the horizontal axis, and x2 serves as the moderator variable. By
default, three hypothetical values of x2 are selected (in this case
the quantiles 25\%, 50\%, and 75\%). The second example in that figure
illustrates user-selected values for the moderator.

\noindent 
\begin{lstlisting}
plotSlopes(m3, plotx="x1", modx="x2", modxVals=c(-0.2, 0.5, 0.9), xlab="Continuous Predictor")
\end{lstlisting}


\begin{figure}
<<ps10, fig=T, echo=F, height=9, width=4>>=
par(mfcol=c(2,1))
m3psa <- plotSlopes(m3, plotx="x1", modx="x2", xlab="x1 is a Continuous Predictor")
m3psb <- plotSlopes(m3, plotx="x1", modx="x2", modxVals=c(0.2, 0.5, 0.7), xlab="Continuous Predictor")
par(mfcol=c(1,1))
@

\caption{plotSlopes In an Additive Model\label{fig:ps10}}
\end{figure}


<<>>=
testSlopes(m3psa)
@

That model is linear, so lines are parallel. We need to introduce
some interaction effects in order to exploit the new functions proposed
here. Suppose we generate a new dependent variable and fit a regression
with an interaction: 

<<ps15>>=
dat$y4 <- with(dat, -3*x1 + 6*x2 - 0.17*x1*x2 + 5*rnorm(100))
m4 <- lm (y4 ~ x1 * x2, data=dat)
@

A figure with lines for some values of the moderator x2, along with
hypothesis test for those estimates, is obtained with the following.
The ``simple slope'' lines that model are presented in Figure \ref{fig:ps20}.

\begin{lstlisting}
m4ps <- plotSlopes(m4, plotx="x1", modx="x2", xlab="Continuous Predictor")
\end{lstlisting}


\begin{figure}


<<ps20, fig=T, echo=T>>=
m4ps <- plotSlopes(m4, plotx="x1", modx="x2", xlab="Continuous Predictor")
@

\caption{plotSlopes for an Interactive Model\label{fig:ps20}}


\end{figure}


Aiken and West (and later Cohen, Cohen, West, and Aiken) propose using
the t test to find out if the effect of the ``plotx'' variable is
statistically significant for each particular value of ``modx,''
the moderator variable. The testSlopes function delivers those t tests.
Each of the lines represents a test of the hypothesis that 
\begin{equation}
H_{0}:0=\hat{b}_{simple\, slope}=\hat{b}_{plotx}+b_{plotx\cdot modx}modx
\end{equation}


\noindent where $modx$ is the numeric value of the moderator variable
and $plotx$ is the variable that is plotted on the horizontal axis
in the plotSlopes output.

Following a suggestion of Preacher, Curran, and Bauer (2006), the
testSlopes function also tries to calculate the Johnson-Neyman (1936)
interpretation of the same test. It presents 2 diagnostic plots, as
illustrated in Figure \ref{fig:ts10}. Whereas West and Aiken would
have us test the hypothesis that $\hat{b}_{simple\, slope}=\hat{b}_{plotx}+b_{plotx\cdot modx}modx$
is different from 0, J-N would have us ask ``for which values of
the moderator would the value $\hat{b}_{simple\, slope}$ be statistically
significantly different from zero? The J-N calculation requires the
solution an equation that is quadratic in the value of the moderator
variable, $modx$. The interval of values of $modx$ associated with
a statistically significant effect of $plotx$ on the outcome is determined
from the computation of a T statistic for $\hat{b}_{simple\, slope}$.
The J-N interval is the set of values of $modx$ for which the following
holds:
\begin{equation}
\hat{t}=\frac{\hat{b}_{simple\, slope}}{std.err(\hat{b}_{simple\, slope})}=\frac{\hat{b}_{simple\, slope}}{\sqrt{\widehat{Var(\hat{b}_{plotx})}+modx^{2}\widehat{Var(\hat{b}_{plotx\cdot modx})}+2modx\widehat{Cov(\hat{b}_{plotx},\hat{b}_{plotx\cdot modx})}}}\geq T_{\frac{\alpha}{2},df}
\end{equation}


\noindent I am not entirely convinced that the J-N interpretation
is useful, but calculating it was interesting. Nevertheless, the output
of testSlopes(m4ps) is displayed in Figure \ref{fig:ts10}.

\begin{figure}
<<ts10, fig=T, echo=T>>=
testSlopes(m4ps)
@

\caption{testSlopes for an Interactive Model\label{fig:ts10}}
\end{figure}


The plotPlane function offers another visualization of the mutual
effect of two predictors in m4. See Figure \ref{fig:pp100}

\begin{figure}
<<pp100, fig=T>>=
p100 <- plotPlane(m4, plotx1="x1", plotx2="x2")
@

\caption{plotPlane for the Interactive Model\label{fig:pp100}}
\end{figure}


At some point in the future, the ability to make plotSlopes and plotPlane
work together will be introduced. So the user will be able to press
the plane down into the 2 dimensional slopes plot, or the simple slopes
can be depicted in the 3 dimensional plane. A preliminary rendering
of what that might look like is presented in Figure \ref{fig:pp110}

\begin{figure}
<<pp110, fig=T, echo=F, width=5, height=4>>=
m4ps <- plotSlopes(m4, plotx="x1", modx="x2", xlab="Continuous Predictor", ylim=c(-25, 35))
@
<<pp111, fig=T, echo=F, height=5>>=
p110 <- plotPlane(m4, plotx1="x1", plotx2="x2", x1lab="Continuous Predictor")
for(j in unique(m4ps$newdata$x2)){
subdat <- m4ps$newdata[m4ps$newdata$x2==j,]
lines(trans3d(subdat$x1, subdat$x2, subdat$pred, pmat=p110$res), col="red", lwd=3)
}
@

\caption{Making plotSlopes and plotPlane work Together\label{fig:pp110}}
\end{figure}



\section{Standardized, Mean-Centered, and Residual-Centered Regressions }


\subsection{Standardized regression}

Many of us learned to conduct regression analysis with SPSS, which
(historically, at least) reported both the ordinary regression coefficients
as well as a column of coefficients obtained from a regression in
which each of the predictors in the design matrix had been ``standardized.''
That is to say, each variable, for example $x1_{i}$, was replaced
by an estimated $Z-score$ $(x1_{i}-\overline{x1})/std.dev.(x1_{i}$).
A regression fitted with those standardized variables is said to produce
``standardized coefficients.'' These standardized coefficients,
dubbed ``beta weights'' in common parlance, were thought to set
different kinds of variables onto a common metric. While this idea
appears to have been in error (see, for example, King 1986), it still
is of interest to many scholars who want to standardize their variables
in order to compare them more easily. 

The function standardize was included in rockchalk to facilitate lectures
about what a researcher ought not do. standardize performs the complete,
mindless standardization of all predictors, no matter whether they
are categorical, interaction terms, or transformed values (such as
logs). Each column of the design matrix is scaled to a new variable
with mean 0 and standard deviation 1. The input to standardize should
be a fitted regression model. For example:

<<>>=
m4 <- lm (y4 ~ x1 * x2, data=dat)
m4s <- standardize(m4)
@

It does seem odd to me that a person would actually want a standardized
regression of that sort, and the commentary included with the summary
method for the standardized regression object probably makes that
clear.

<<>>=
summary(m4s)
@

\begin{table}
\caption{Comparing Ordinary and Standardized Regression\label{tab:stdreg10}}


<<stdreg10, results=tex, echo=F>>=
outreg(list(m4,m4s), tight=F, modelLabels=c("Not Standardized","Standardized"))
@
\end{table}



\subsection{Mean-centering}

In contrast with a standardized regression, a mean-centered regression
is one in which one or more of the predictors has been ``mean centered''
before the design matrix is constructed. The rockchalk package includes
a meanCenter function that can, center some or all of the predictors
before the design matrix is constructed. It can also standardize those
variables \emph{before} the design matrix is constructed.

Does a regression model with mean-centered predictors have better
statistical properties than a regression that uses the variables as
they are originally presented. Some, most notably Aiken and West (1991)
and Cohen, et al. (2002) argued that the answer is an emphatic ``yes.''
In retrospect, it appears this advice was mistaken, especially where
the amelioration of multicollinearity is the primary purpose (Echambadi
and Hess (2007)). Nevertheless, the issue is of more than passing
interest to many applied researchers, who have experienced the frustration
of having their results ``destroyed'' by the inclusion of additional
terms involving products of variables that are already in their models.

It is often noted (by researchers and students alike) that the estimates
of the ordinary linear regression are affected in surprising ways
by the introduction of nonlinear expressions. Suppose we begin with
an ordinary linear model.

\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+e_{i}\label{eq:linear}
\end{equation}


\noindent Then we add, for example, a squared term, 
\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3x2_{i}^{2}+e_{i}
\end{equation}


\noindent or an ``interaction effect'', 

\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3(x1_{i}\cdot x2_{i})+e_{i}
\end{equation}


In both of these cases, practitioners have long been bothered by the
fact that the estimate of $b_{1}$ or $b_{2}$ in model (\ref{eq:linear})
might be ``statistically significant'' (significantly different
from 0, that is), but when the last term is added, the standard errors
of the estimates grow larger and ``nothing is significant anymore.'' 

Aiken and West (1991) and Cohen, et. al (2002) contended that the
apparent instability of the coefficients is a reflection of ``inessential
collinearity'' among the predictors, due to the fact that $x1$ and
$x2$ are in fact correlated with the new terms, $x2^{2}$ or $x1\cdot x2$.
Their recommendation is that practitioners ought to mean-center those
predictors, to replace $x1$ by $(x1_{i}-\overline{x1})$ and $(x2_{i}-\overline{x2})$.
In some cases, it appears as though the use of mean-centered variables
does indeed address the multicollinearity problem, making the t-statistics
look ``bigger'' and the p values are smaller.

While the superficial evidence for mean-centering seemed compelling,
it turns out to be a complete mirage. Mean-centering does not solve
the problem of multicollinearity, it merely changes the point at which
we evaluate it. This point is made most emphatically by Echambadi
and Hess (2007), who argue that mean-centering has no effect (not
one ``iota'' of an effect!) on multicollinearity. 

In order to help students and researchers explore this controversy,
the rockchalk includes the function meanCenter. Mean-centering makes
it easier to tell, at a glance, the model's predicted value for the
case that is situated at the mean.

For this example, I use a function to generate data called genCorrelatedData.
It is included with rockchalk. The ``true model'' from which the
data is produced is
\begin{equation}
y_{i}=2+0.1x1_{i}+0.1x2_{i}+0.2\cdot(x1_{i}\times x2_{i})+e_{i}
\end{equation}


The usual ``course of affairs'' would observe the following sequence
of events. Three regressions will be estimated, they are summarized
in Table \ref{tab:meancenter10-1}. First (the first column), the
researcher ``explores'' a linear specification,

\begin{lstlisting}
lm(y ~ x1 + x2, data=dat2)
\end{lstlisting}


\noindent The coefficients of $x1$ and $x2$ appear to be statistically
significant, a very gratifying regression indeed. 

\noindent Second (the second column in Table \ref{tab:meancenter10-1}),
an interaction term is added to the model. That interaction term,
the product variables $x1\times x2$, is estimated in R with

\begin{lstlisting}
lm(y ~ x1 * x2, data=dat2)
\end{lstlisting}


\noindent This specification leads to a model that includes the main
effect variables $x1$ and $x2$, as well as their product, which
is labeled $x1:x2$ in the output. When most of us see that second
column for the first time, we think ``Holy Cow! My regression went
to hell!'' The situation does appear dire. While the coefficients
for the variables $x1$ and $x2$ did seem to be substantial in the
first model, the introduction of the interactive effect renders everything
statistically insignificant. 

Now comes the ``magic'' of mean centering. If we replace $x1$ and
$x2$ with their mean centered counterparts, and then calculate the
interaction variable as the product of those two centered variables,
we get a ``great'' regression, which is presented in the third column
of Table \ref{tab:meancenter10-1}. Everything appears to be significant,
order has been restored in the land of the more-or-less linear model.

<<echo=F, include=F>>=
dat2 <- genCorrelatedData(N=400, rho=.4, stde=300, beta=c(2,0.1,0.1,0.2))
m4linear <- lm (y ~ x1 + x2, data=dat2)
m4int <- lm (y ~ x1 * x2, data=dat2)
m4mc <- meanCenter(m4int)
@

\begin{table}
\caption{Comparing Ordinary and Mean-Centered Regression\label{tab:meancenter10-1}}


<<mcenter10, results=tex, echo=F>>=
outreg(list(m4linear, m4int,m4mc), tight=F, modelLabels=c("Linear", "Not Centered","Mean Centered"))
@
\end{table}


There is a good argument (actually an invincible argument), that the
mean-centering effect is a complete and total illusion. The first
piece of evidence should be that the coefficient of the interactive
effect in columns 2 and 3 is identical. The root mean square and $R^{2}$
estimates are identical. And, if we look into the situation a little
more closely, we find that the models produce identical predicted
values! The 3 dimensional plots of the predicted values of the two
models are compared in Figure \ref{fig:mcenter30}. 
\begin{figure}
<<mcenter50, fig=T,echo=FALSE, height=5, width=7>>=
op <- par()
par(mfcol=c(1,2))
par(mar=c(2,2,2,1))
plotPlane(m4int, plotx1="x1", plotx2="x2", plotPoints=FALSE, main="Not Centered", ticktype="detailed")
plotPlane(m4mc, plotx1="x1", plotx2="x2", plotPoints=FALSE, main="Mean Centered", ticktype="detailed")
par(op)
@

\caption{Mean Centered and Uncentered Fits Identical\label{fig:mcenter30}}
\end{figure}


The curves in Figure \ref{fig:mcenter30} are identical, of course.
The only difference is that the one on the left, the one for the original
non-transformed data, has the ``y axis'' positioned at the front-left
edge of the graph, while the centered one re-positions the y axis
into the center of the data. There are two reasons why the mean-centering
``seems to'' help multicollinearity, when in fact it has no effect
at all. First, an interaction model is a nonlinear model. The slope
of an effect is different at every point in the X1,X2 plane. This,
of course, means that multicollinearity is not a global attribute
of the data, but rather it is a local attribute, so that the effect
of multicollinearity is more obvious when the slope is flat than when
it is steep. Since the regression fit measures multicollinearity at
the origin, where the y axis is ``stuck into the ground,'' it only
makes sense that re-positioning the axis would affect our assessment
of collinearity. Second, the standard error of predicted values is
hour-glass shaped--smaller in the center of the data cloud, wider
at the edges. Students of elementary regression have no-doubt seen
the confidence interval plot similar to Figure \ref{fig:Hourglass}.
The ``mean centered'' regression is a snapshot of the standard errors
in the small part of the hourglass, while the non-centered regression
is a snapshot at the outer edge. They are, of course, the exact same
model, and the results differ only superficially. 

\begin{figure}


<<fig=T, echo=F, height=5, width=5>>=
x <- rnorm(100, m=50, s=20)
y <- 3 + 0.2 * x + 15 * rnorm(100)
plot(x,y)
mp1 <- lm(y ~ x)
abline(mp1)
ndf <- data.frame(x=plotSeq(x,40))
p1 <- as.data.frame(predict(mp1, interval="conf", newdata = ndf))
lines(ndf$x, p1$fit, col="black", lwd=2)
lines(ndf$x, p1$lwr, col="red", lwd=2, lty=4)
lines(ndf$x, p1$upr, col="red", lwd=2, lty=3)
legend("topleft", legend=c("predicted","conf. lower", "conf. upper"), col=c("black","red","red"), lty=c(1,4,3))
@

\caption{The Hourglass\label{fig:Hourglass}}


\end{figure}


Included with the rockchalk package, in the examples folder, one can
find a file called ``residualCentering.R'' that walks through this
argument step by step. In addition, I have several lectures for an
intermediate regression class on this issue and they can be found
under \url{http://pj.freefaculty.org/guides/stat}. 


\subsection{Residual-centering}

The argument against mean-centering is that it makes absolutely no
difference. Perhaps the same cannot be said of mean-centering's cousin,
``residual-centering.'' 

Residual-centering is another way to deal with the problem that the
constructed variable representing the interaction, ``$x1\times x2$,''
will sometimes cause multicollinearity, exaggerating standard errors,
making t-statistics small and p-values big. 

We would still like to estimate a model

\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3(x1_{i}\times x2_{i})+e_{i},\label{eq:rc10}
\end{equation}


\noindent but we can't. More accurately, we can estimate it, but we
don't like the results we get. We don't like the large standard errors
and huge p-values.

So here's the plan. If we fit the linear model--with no interaction

\begin{equation}
y=c_{0}+c_{1}x1+c_{2}x2+e2_{i}\label{eq:rc20}
\end{equation}


\noindent we get parameter estimates that we like for the effects
of $x1$ and $x2$. We will proceed by constraining the fitted coefficients
in the full, interactive model so that the main effects remain the
same. That is to say, fit \ref{eq:rc10}, but force the fit so that
the coefficients of $x1$ and $x2$ match equation \ref{eq:rc20}.
Effectively, we impose the restriction that $\hat{b}_{1}=\hat{c}_{1}$
and $\hat{b}_{2}=\hat{c}_{2}$.

How can this be done in a convenient, practical way? We need an estimate
of the interaction effect that is orthogonal to (uncorrelated with)
both $x1$ and $x2$. That's where residual-centering comes into the
picture. Estimate the following regression, in which the left hand
side is the interaction product term:
\begin{equation}
(x1_{i}\times x2_{i})=d_{0}+d_{1}x1_{i}+d_{2}x2+u_{i}\label{eq:residCentered}
\end{equation}


The residuals from that regression are, by definition, orthogonal
to both $x1$ and $x2$. Let's use that residual as a new indicator
of the interaction effect. Call it $(x1Xx2).$ Then we fit an equation
like (\ref{eq:rc10}), but instead of the actual product term $(x1_{i}\times x2_{i})$,
we include the residual from the fitted regression (\ref{eq:residCentered}). 

\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3(x1Xx2)+e_{i},\label{eq:rc10-1}
\end{equation}


\noindent In essence, we have taken the interaction $(x1_{i}\times x2_{i})$,
and purged it of its parts that are linearly related to $x1_{i}$
and $x2_{i}$ separately. The ``residual centered'' regression adds
the new variable, $(x1Xx2)$, and it leaves the effect coefficients
for variables $x1$ and $x2$ unchanged. 

Table \ref{tab:residcenter10} compares the parameter estimates of
the models with interaction terms. One is the usual, non-centered,
model, and the others are the mean-centered and residual-centered
estimates. One peculiar, important, fact is that the estimate of the
interaction coefficient is the same in all three models. They are
not just similar. They are identical. The fact that the usual and
mean-centered estimates are the same has, of course, been noted in
Cohen, et al (2002), but the significance was missed. As argued in
the previous section of this essay, the two models have more in common
than just that one coefficient. The models--all three of them-- are
actually identical. 

<<echo=F, include=F>>=
dat2 <- genCorrelatedData(N=400, rho=.4, stde=300, beta=c(2,0.1,0.1,0.8))
m4linear <- lm (y ~ x1 + x2, data=dat2)
m4int <- lm (y ~ x1 * x2, data=dat2)
m4mc <- meanCenter(m4int)
m4rc <- residualCenter(m4int)
@

\begin{table}
\caption{Comparing Ordinary and Residual-Centered Regression\label{tab:residcenter10}}


<<rcenter10, results=tex, echo=F>>=
outreg(list(m4linear, m4int, m4mc, m4rc), tight=T, modelLabels=c("Linear", "Not Centered","Mean Centered", "Resid Centered"))
@
\end{table}


Does the identicality of the interaction estimates indicate that the
residual-centered regression is also equivalent to the mean-centered
and usual specifications? It appears the answer is, ``yes.'' Emphatically.
In order to make this perfectly clear, we need to calculate predicted
values for a residual-centered regression model. In rockchalk-1.5.1,
a predict method was introduced for that purpose. To calculate a prediction,
it is necessary to specify the values of all of the predictors, and
then the fitted models for each of the interactions is used to calculate
residuals that can be used as interaction terms in the final model. 

With predicted values in hand, we can demonstrate the fact that the
predicted values from all three methods of estimating interactions
are identical. In Figure \ref{fig:rcenter30}, the three dimensional
plots of the three models are presented together. For a given pair
of input values $x1_{i}$ and $x2_{i}$, all three models offere the
same prediction. From that, one must conclude that the three regression
specifications are, despite the superficial differences among their
estimated coefficients, actually the same. 

Why do the coefficients differ if the models are actually the same?
Recall that we are estimating the slopes of a curving plane, and so
estimates of the marginal effects of $x1$ and $x2$ will depend on
the point at which we are calculating the slopes. Mean-centering and
residual-centering are simply competing methods for re-positioning
the $y$ axis. The interactive model has a constant mixed partial
derivative, so the estimate of the interaction coefficient is the
same in all three models. The similarity of the three plots in Figure
\ref{fig:rcenter30} was, in all honesty, a surprise. Intuitively,
it seems as though the residual-centered approach is different because
the data that represents the interaction is drawn from the residuals
of a preliminary regression. We have methodically re-calculated the
predicted values from the residual centered regression. Through whatever
method one chooses, the results are the same. 

\begin{figure}
<<rcenter30, fig=T,echo=FALSE, height=9, width=6>>=
op <- par()
par(mfrow=c(2,2))
par(mar=c(2,2,2,1))
plotPlane(m4int, plotx1="x1", plotx2="x2", plotPoints=TRUE, main="Not Centered", ticktype="detailed", theta=-20)
plotPlane(m4mc, plotx1="x1", plotx2="x2", plotPoints=TRUE, main="Mean Centered", ticktype="detailed", theta=-20)
plotPlane(m4rc, plotx1="x1", plotx2="x2", plotPoints=TRUE, main="Residual Centered", ticktype="detailed", theta=-20)
par(op)
@

\caption{Mean Centered and Uncentered Fits Identical\label{fig:rcenter30}}
\end{figure}


\begin{figure}
<<rcenter40, fig=T,echo=FALSE, height=4, width=6>>=
m4mcpred <- predict(m4mc, newdata=dat2)
m4rcpred <- predict(m4rc, newdata=dat2)
plot(m4mcpred, m4rcpred, main="", xlab="Predictions of Mean-centered Regression",ylab="Predictions from Residual-centered Regression")
predcor <- round(cor(m4mcpred, m4rcpred),3)
legend("topleft", legend=c(paste("Correlation=", predcor)))
@

\caption{Predicted Values of Mean and Residual-centered Models\label{fig:rcenter40}}
\end{figure}


In Figure \ref{fig:rcenter40}, the predicted values of the mean-centered
and residual-centered regressions are plotted against one another.
They are perfectly co-incident, as evidenced by the fact that the
correlation between them is 1.0. 

The conclusion is this. One can code a nonlinear model in various
ways, all of which are theoretically and analytically identical. There
are superficial differences in the estimates of the coefficients of
the various specifications, but these differences are understandable
in light of the changes in the design matrix. The connection between
the observed values of the predictors and the predicted values remains
the same in all of these specifications.

Consider the following derivation. The ordinary model is
\begin{equation}
y_{i}=b_{0}+b_{1}x1_{i}+b_{2}x2_{i}+b3(x1_{i}\times x2_{i})+e1_{i}\label{eq:int}
\end{equation}


The mean-centered model is 
\begin{equation}
y_{i}=c_{0}+c_{1}(x1_{i}-\overline{x1})+c_{2}(x2_{i}-\overline{x2})+c_{3}(x1_{i}-\overline{x1})\times(x2_{i}-\overline{x2})+e2_{i}\label{eq:mc1}
\end{equation}


In order to compare with equation \ref{eq:int}, we would re-arrange
like so

\begin{equation}
y_{i}=c_{0}+c_{1}(x1_{i})-c_{1}\overline{x1}+c_{2}(x2_{i})-c_{2}\overline{x2}+c_{3}(x1_{i}x2_{i}+\overline{x1}\overline{x2}-\overline{x1}x2_{i}-\overline{x2}x1_{i})+e2_{i}
\end{equation}


\begin{equation}
y_{i}=c_{0}+c_{1}(x1_{i})-c_{1}\overline{x1}+c_{2}(x2_{i})-c_{2}\overline{x2}+c_{3}(x1_{i}x2_{i})+c_{3}\overline{x1}\overline{x2}-c_{3}\overline{x1}x2_{i}-c_{3}\overline{x2}x1_{i})+e2_{i}
\end{equation}


\begin{equation}
y_{i}=\{c_{0}-c_{1}\overline{x1}-c_{2}\overline{x2}+c_{3}\overline{x1}\overline{x2}\}+\{c_{1}-c_{3}\overline{x2}\}x1_{i}+\{c_{2}-c_{3}\overline{x1}\}x2_{i}+c_{3}(x1_{i}x2_{i})+e2_{i}\label{eq:mc3}
\end{equation}


One can then compare the parameter estimates from equations \ref{eq:int}
and \ref{eq:mc3} in order to understand the observed changes in fitted
coefficients after changing from the ordinary to the mean-centered
coding. Both \ref{eq:int} and \ref{eq:mc3} include a single parameter
times $(x1_{i}x2_{i}),$ leading one to expect that the estimate $\hat{b}_{3}$
should be equal to the estimate of $\hat{c}_{3}$ (and they are, as
we have found). Less obviously, one can use the fitted coefficients
from either model to deduce the fitted coefficients from the other.
The following equalities describe that relationship.
\begin{eqnarray}
\hat{b}_{0} & = & \hat{c}_{0}-\hat{c}_{1}\overline{x1}-\hat{c}_{2}\overline{x2}+\hat{c}_{3}\overline{x1}\overline{x2}\\
\hat{b}_{1} & = & \hat{c}_{1}-\hat{c}_{3}\overline{x2}\\
\hat{b}_{2} & = & \hat{c}_{2}-\hat{c}_{3}\overline{x1}\\
\hat{b}_{3} & = & \hat{c}_{3}
\end{eqnarray}
The estimated fit of equation \ref{eq:mc1} would provide estimated
coefficients $\hat{c}_{j}$, $j=0,...,3$, which would then be used
to calculate the estimates from the noncentered model.

The residual centered model requires two steps. First, estimate a
regression 
\begin{equation}
(x1_{i}\times x2_{i})=d_{0}+d_{1}x1_{i}+d_{2}x2_{i}+u_{i}
\end{equation}


\noindent from which the predicted value can be calculated:
\begin{equation}
\widehat{(x1_{i}\times x2_{i})}=\hat{d}_{0}+\hat{d}_{1}x1_{i}+\hat{d}_{2}x2_{i}
\end{equation}


\noindent The difference between the observed product, $x1_{i}\times x2_{i}$
and the predicted value from that model, $\widehat{x1_{i}\times x2_{i}}$,
is the ``residual-centered estimate,'' which is used as a predictor
in place of $(x1_{i}\times x2_{i})$ in equation \ref{eq:int}. 

\begin{equation}
y_{i}=h_{0}+h_{1}x1_{i}+h_{2}x2_{i}+h_{3}\{x1_{i}\times x2_{i}-\widehat{x1_{i}\times x2_{i}}\}+e3_{i}\label{eq:rc1}
\end{equation}


Replacing $\widehat{x1_{i}\times x2_{i}}$ with $\hat{d}_{0}+\hat{d}_{1}x1_{i}+\hat{d}_{2}x2_{i}$

\begin{eqnarray}
y_{i} & = & h_{0}+h_{1}x1_{i}+h_{2}x2_{i}+h_{3}\{x1_{i}\times x2_{i}-\hat{d}_{0}-\hat{d}_{1}x1_{i}-\hat{d}_{2}x2_{i}\}+e3_{i}\\
 & = & h_{0}+h_{1}x1_{i}+h_{2}x2_{i}+h_{3}\{x1_{i}\times x2_{i}\}-h_{3}\hat{d}_{0}-h_{3}\hat{d}_{1}x1_{i}-h_{3}\hat{d}_{2}x2_{i}\}+e3_{i}\\
 &  & \{h_{0}-h_{3}\hat{d}_{0}\}+\{h_{1}-h_{3}\hat{d}_{1}\}x1_{i}+\{h_{2}-h_{3}\hat{d}_{2}\}x2_{i}+h_{3}\{x1_{i}\times x2_{i}\}+e3_{i}
\end{eqnarray}


As in the previous comparison of models, we can translate coefficient
estimates between the ordinary specification and the residual-centered
model. The coefficient estimated for the product term, $\hat{h}_{3}$,
should be equal to $\hat{b}_{3}$ and $\hat{c}_{3}$ (and it is!).
If we fit the residual centered model, \ref{eq:rc1}, we can re-generate
the coefficients of the other models like so: 
\begin{eqnarray}
b_{0} & =\hat{c}_{0}-\hat{c}_{1}\overline{x1}-\hat{c}_{2}\overline{x2}+\hat{c}_{3}\overline{x1}\overline{x2}= & h_{0}-h_{3}\hat{d}_{0}\\
b_{1} & =\hat{c}_{1}-\hat{c}_{3}\overline{x2}= & h_{1}-h_{3}\hat{d}_{1}\\
b_{2} & =\hat{c}_{2}-\hat{c}_{3}\overline{x1}= & h_{2}-h_{3}\hat{d}_{2}
\end{eqnarray}


Little, T. D., Bovaird, J. A., and Widaman, K. F. (2006). On the Merits
of Orthogonalizing Powered and Product Terms: Implications for Modeling
Interactions Among Latent Variables. Structural Equation Modeling,
13(4), 497-519.
\end{document}
